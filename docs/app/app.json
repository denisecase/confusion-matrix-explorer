[{"name": "app.py", "content": "# app_raise_the_bar.py\n\"\"\"Explore how changing the decision threshold (the vertical T line) affects the confusion matrix and related metrics (sensitivity, specificity, etc.) for a binary classification problem.\"\"\"\n\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px  # pyright: ignore[reportMissingTypeStubs]\nimport plotly.graph_objects as go  # pyright: ignore[reportMissingTypeStubs]\nfrom plotly.graph_objects import Figure  # pyright: ignore[reportMissingTypeStubs]\nfrom shiny import reactive, render\nfrom shiny.express import input as shiny_input\nfrom shiny.express import ui\nfrom shinywidgets import render_plotly\n\n# Use local imports\nfrom utils_confusion import (\n    ConfusionCounts,\n    ConfusionMetrics,\n    compute_confusion_counts,\n    compute_metrics,\n    format_counts_table,\n    generate_synthetic_scores,\n)\n\n# -----------------------------\n# Global data (synthetic scores)\n# -----------------------------\n\n# Teaching, not research, just generate once at startup.\nBASE_DF: pd.DataFrame = generate_synthetic_scores(\n    n_pos=200,\n    n_neg=200,\n    pos_mean=1.5,\n    neg_mean=0.0,\n    pos_std=1.0,\n    neg_std=1.0,\n    random_state=42,\n)\n\n\n# -----------------------------\n# UI\n# -----------------------------\n\nui.page_opts(title=\"Confusion Matrix Explorer\")\n\nui.markdown(\n    \"\"\"\n    - **Left** of the T Line is \"Test negative\" | **Right** of the T Line is \"Test positive\".\n    - **Raising the bar** (moving threshold right):\n      - makes it harder to call someone positive: sensitivity goes down, **specificity goes up**.\n    - **Lowering the bar** (moving threshold left):\n      - makes it easier to call someone positive: **sensitivity goes up**, specificity goes down.\n\n    \"\"\"\n)\n\nwith ui.sidebar():\n    ui.h4(\"Threshold and Distribution Settings\")\n\n    ui.input_slider(\n        \"threshold\",\n        \"Decision threshold (raise the bar)\",\n        min=float(BASE_DF[\"score\"].min()),\n        max=float(BASE_DF[\"score\"].max()),\n        value=float(BASE_DF[\"score\"].mean()),\n        step=0.1,\n    )\n\n    ui.input_checkbox(\n        \"show_density\",\n        \"Normalize histograms (density)\",\n        value=True,\n    )\n\n    ui.input_slider(\n        \"bins\",\n        \"Number of bins\",\n        min=10,\n        max=60,\n        value=30,\n        step=5,\n    )\n\n    ui.markdown(\n        \"\"\"\n        - Move the decision threshold to **raise** or **lower** the bar.\n        - Watch how TP, FP, FN, TN and metrics change.\n        \"\"\"\n    )\n\n\n@reactive.calc\ndef current_threshold() -> float:\n    \"\"\"Get the current decision threshold value from the slider input.\n\n    Returns:\n    float\n        The current threshold value selected by the user.\n    \"\"\"\n    return float(shiny_input.threshold())\n\n\nwith ui.layout_columns(col_widths=(6, 6)):\n    with ui.card():\n        ui.h4(\"Score Distributions and Threshold\")\n\n        @render_plotly\n        def score_histogram() -> Figure:\n            \"\"\"Overlaid histograms of scores for Disease Present vs Absent.\n\n            Creates overlaid histograms with a vertical line at the current threshold.\n            \"\"\"\n            threshold = current_threshold()\n            density = bool(shiny_input.show_density())\n            bins = int(shiny_input.bins())\n\n            # Copy to avoid accidental mutation\n            df = BASE_DF.copy()\n\n            # Map label to a friendly name\n            mapping: dict[int, str] = {1: \"Disease Present\", 0: \"Disease Absent\"}\n            df[\"group\"] = df[\"label\"].replace(mapping)\n\n            fig = px.histogram(\n                df,\n                x=\"score\",\n                color=\"group\",\n                nbins=bins,\n                histnorm=\"probability\" if density else None,\n                barmode=\"group\",  # group for side-by-side bars, overlay for overlapping\n                opacity=0.6,\n                labels={\"score\": \"Test score\", \"group\": \"Group\"},\n            )\n\n            # Add a vertical line for the threshold\n            fig.add_vline(\n                x=threshold,\n                line_width=2,\n                line_dash=\"dash\",\n            )\n\n            fig.update_layout(\n                title={\n                    \"text\": \"Score distributions with decision threshold\",\n                    \"y\": 0.99,  # move title up (0\u20131 scale)\n                    \"x\": 0.5,\n                    \"xanchor\": \"center\",\n                    \"yanchor\": \"top\",\n                },\n                margin={\"t\": 80, \"b\": 40, \"l\": 40, \"r\": 40},  # add top/bottom/side margins\n                legend={\n                    \"orientation\": \"h\",\n                    \"yanchor\": \"bottom\",\n                    \"y\": 1.02,\n                    \"xanchor\": \"right\",\n                    \"x\": 1.0,\n                },\n            )\n\n            return fig\n\n    with ui.card():\n        ui.h4(\"Confusion Matrix and Metrics\")\n\n        @reactive.calc\n        def current_counts_and_metrics() -> tuple[ConfusionCounts, ConfusionMetrics]:\n            \"\"\"Calculate confusion matrix counts and metrics for the current threshold.\n\n            Returns:\n            tuple[ConfusionCounts, ConfusionMetrics]\n                A tuple containing the confusion matrix counts and computed metrics.\n            \"\"\"\n            threshold = current_threshold()\n            scores = BASE_DF[\"score\"].to_numpy()\n            labels = BASE_DF[\"label\"].to_numpy()\n\n            counts = compute_confusion_counts(scores, labels, threshold)\n            metrics = compute_metrics(counts)\n            return counts, metrics\n\n        @render.table\n        def confusion_table():\n            \"\"\"Generate a formatted confusion matrix table for display.\n\n            Returns:\n                Formatted confusion matrix table showing TP, FP, FN, TN counts.\n            \"\"\"\n            counts, _ = current_counts_and_metrics()\n            return format_counts_table(counts)\n\n        @render.ui\n        def metrics_table() -> ui.HTML:\n            \"\"\"Generate an HTML table for metrics display.\"\"\"\n            _, metrics = current_counts_and_metrics()\n\n            def pct(x: float) -> str:\n                if np.isnan(x):\n                    return \"N/A\"\n                return f\"{x * 100:.1f}%\"\n\n            return ui.HTML(f\"\"\"\n                <table style=\"width: 100%; border-collapse: collapse;\">\n                    <tr style=\"border-bottom: 1px solid #ddd;\">\n                        <td style=\"padding: 8px; font-weight: bold;\">Sensitivity (TPR)</td>\n                        <td style=\"padding: 8px; text-align: right;\">{pct(metrics.sensitivity)}</td>\n                    </tr>\n                    <tr style=\"border-bottom: 1px solid #ddd;\">\n                        <td style=\"padding: 8px; font-weight: bold;\">Specificity (TNR)</td>\n                        <td style=\"padding: 8px; text-align: right;\">{pct(metrics.specificity)}</td>\n                    </tr>\n                    <tr style=\"border-bottom: 1px solid #ddd;\">\n                        <td style=\"padding: 8px; font-weight: bold;\">Precision (PPV)</td>\n                        <td style=\"padding: 8px; text-align: right;\">{pct(metrics.precision)}</td>\n                    </tr>\n                    <tr style=\"border-bottom: 1px solid #ddd;\">\n                        <td style=\"padding: 8px; font-weight: bold;\">Negative PV (NPV)</td>\n                        <td style=\"padding: 8px; text-align: right;\">{pct(metrics.npv)}</td>\n                    </tr>\n                    <tr style=\"border-bottom: 1px solid #ddd;\">\n                        <td style=\"padding: 8px; font-weight: bold;\">Accuracy</td>\n                        <td style=\"padding: 8px; text-align: right;\">{pct(metrics.accuracy)}</td>\n                    </tr>\n                    <tr>\n                        <td style=\"padding: 8px; font-weight: bold;\">Prevalence</td>\n                        <td style=\"padding: 8px; text-align: right;\">{pct(metrics.prevalence)}</td>\n                    </tr>\n                </table>\n            \"\"\")\n\n\nwith ui.card():\n    ui.h4(\"Confusion Heatmap\")\n\n    @render_plotly\n    def confusion_heatmap() -> Figure:\n        \"\"\"Generate an interactive confusion matrix heatmap.\"\"\"\n        counts, _ = current_counts_and_metrics()\n\n        # Create matrix for heatmap\n        matrix = np.array([[counts.tn, counts.fp], [counts.fn, counts.tp]])\n\n        # Annotations for each cell\n        annotations = []\n        labels = [[\"TN\", \"FP\"], [\"FN\", \"TP\"]]\n\n        for i in range(2):\n            for j in range(2):\n                annotations.append(\n                    {\n                        \"text\": f\"<b>{labels[i][j]}</b><br>{matrix[i][j]}\",\n                        \"x\": j,\n                        \"y\": i,\n                        \"xref\": \"x\",\n                        \"yref\": \"y\",\n                        \"showarrow\": False,\n                        \"font\": {\n                            \"size\": 16,\n                            \"color\": \"white\" if matrix[i][j] > matrix.max() / 2 else \"black\",\n                        },\n                    }\n                )\n\n        fig = go.Figure(\n            data=go.Heatmap(\n                z=matrix,\n                x=[\"Predicted Negative\", \"Predicted Positive\"],\n                y=[\"Actual Negative\", \"Actual Positive\"],\n                colorscale=\"Greens\",  # Or 'Blues' or 'Greens' for correct, 'Reds' for errors\n                showscale=True,\n                hovertemplate=\"%{z} cases<extra></extra>\",\n            )\n        )\n\n        fig.update_layout(\n            annotations=annotations,\n            xaxis={\"side\": \"bottom\"},\n            yaxis={\"autorange\": \"reversed\"},\n            height=400,\n            margin={\"l\": 50, \"r\": 10, \"t\": 10, \"b\": 60},\n        )\n\n        return fig\n\n\nwith ui.card():\n    ui.h4(\"Confusion Bubble Matrix\")\n\n    @render_plotly\n    def confusion_bubble_matrix() -> Figure:\n        \"\"\"Generate a bubble matrix where size represents count.\"\"\"\n        counts, _ = current_counts_and_metrics()\n\n        # Prepare data\n        data = [\n            {\"x\": \"Pred Neg\", \"y\": \"Act Neg\", \"count\": counts.tn, \"label\": \"TN\", \"color\": \"green\"},\n            {\"x\": \"Pred Pos\", \"y\": \"Act Neg\", \"count\": counts.fp, \"label\": \"FP\", \"color\": \"red\"},\n            {\"x\": \"Pred Neg\", \"y\": \"Act Pos\", \"count\": counts.fn, \"label\": \"FN\", \"color\": \"orange\"},\n            {\n                \"x\": \"Pred Pos\",\n                \"y\": \"Act Pos\",\n                \"count\": counts.tp,\n                \"label\": \"TP\",\n                \"color\": \"darkgreen\",\n            },\n        ]\n\n        df_matrix = pd.DataFrame(data)\n\n        # Scale circles - max size proportional to sqrt of count\n        max_count = df_matrix[\"count\"].max()\n        df_matrix[\"size\"] = np.sqrt(df_matrix[\"count\"] / max_count) * 100\n\n        fig = px.scatter(\n            df_matrix,\n            x=\"x\",\n            y=\"y\",\n            size=\"size\",\n            color=\"color\",\n            text=\"label\",\n            hover_data={\"count\": True, \"size\": False, \"color\": False},\n            color_discrete_map={\n                \"green\": \"#2ca02c\",\n                \"darkgreen\": \"#006400\",\n                \"red\": \"#d62728\",\n                \"orange\": \"#ff7f0e\",\n            },\n        )\n\n        # Add count labels\n        fig.update_traces(\n            texttemplate=\"<b>%{text}</b><br>%{customdata[0]}\",\n            textposition=\"middle center\",\n            marker={\"line\": {\"width\": 2, \"color\": \"white\"}},\n        )\n\n        fig.update_layout(\n            showlegend=False,\n            xaxis_title=\"Prediction\",\n            yaxis_title=\"Actual\",\n            yaxis={\"autorange\": \"reversed\"},\n            height=350,\n        )\n\n        return fig\n\n\nwith ui.card():\n    ui.h4(\"Enhanced Confusion Matrix\")\n\n    @render_plotly\n    def enhanced_confusion_matrix() -> Figure:\n        \"\"\"Multi-encoding confusion matrix with size, opacity, and color.\"\"\"\n        counts, _ = current_counts_and_metrics()\n\n        # Create the data structure\n        matrix_data = [\n            {\n                \"row\": \"Actual: No Disease\",\n                \"col\": \"Predicted: No Disease\",\n                \"value\": counts.tn,\n                \"label\": \"TN\",\n                \"correct\": True,\n            },\n            {\n                \"row\": \"Actual: No Disease\",\n                \"col\": \"Predicted: Disease\",\n                \"value\": counts.fp,\n                \"label\": \"FP\",\n                \"correct\": False,\n            },\n            {\n                \"row\": \"Actual: Disease\",\n                \"col\": \"Predicted: No Disease\",\n                \"value\": counts.fn,\n                \"label\": \"FN\",\n                \"correct\": False,\n            },\n            {\n                \"row\": \"Actual: Disease\",\n                \"col\": \"Predicted: Disease\",\n                \"value\": counts.tp,\n                \"label\": \"TP\",\n                \"correct\": True,\n            },\n        ]\n\n        df = pd.DataFrame(matrix_data)\n\n        # Calculate proportions for visual encoding\n        total = df[\"value\"].sum()\n        df[\"proportion\"] = df[\"value\"] / total\n        df[\"opacity\"] = 0.3 + (df[\"value\"] / df[\"value\"].max()) * 0.7\n\n        # Create grid positions\n        positions = {\n            (\"Actual: No Disease\", \"Predicted: No Disease\"): (0, 0),\n            (\"Actual: No Disease\", \"Predicted: Disease\"): (1, 0),\n            (\"Actual: Disease\", \"Predicted: No Disease\"): (0, 1),\n            (\"Actual: Disease\", \"Predicted: Disease\"): (1, 1),\n        }\n\n        fig = go.Figure()\n\n        for _, row in df.iterrows():\n            row_label = str(row[\"row\"])\n            col_label = str(row[\"col\"])\n            x, y = positions[(row_label, col_label)]\n\n            # Size based on count\n            marker_size = 20 + (row[\"proportion\"] * 180)\n\n            # Color based on correct/incorrect\n            is_correct = bool(row[\"correct\"])\n            if is_correct:\n                color = f\"rgba(46, 160, 44, {row['opacity']:.2f})\"  # greenish\n            else:\n                color = f\"rgba(214, 39, 40, {row['opacity']:.2f})\"  # reddish\n            fig.add_trace(\n                go.Scatter(\n                    x=[x],\n                    y=[y],\n                    mode=\"markers+text\",\n                    marker={\n                        \"size\": marker_size,\n                        \"color\": color,\n                        \"line\": {\"width\": 2, \"color\": \"white\"},\n                    },\n                    text=f\"<b>{row['label']}</b><br>{row['value']}<br>({row['proportion'] * 100:.1f}%)\",\n                    textposition=\"middle center\",\n                    textfont={\n                        \"size\": 12 + row[\"proportion\"] * 20,\n                        \"color\": \"white\" if row[\"opacity\"] > 0.6 else \"black\",\n                    },\n                    hovertemplate=f\"{row['label']}: {row['value']} cases<br>\"\n                    + f\"Proportion: {row['proportion'] * 100:.1f}%<extra></extra>\",\n                    showlegend=False,\n                )\n            )\n\n        fig.update_layout(\n            xaxis={\n                \"tickvals\": [0, 1],\n                \"ticktext\": [\"Predicted: No Disease\", \"Predicted: Disease\"],\n                \"range\": [-0.5, 1.5],\n                \"zeroline\": False,\n            },\n            yaxis={\n                \"tickvals\": [0, 1],\n                \"ticktext\": [\"Actual: No Disease\", \"Actual: Disease\"],\n                \"range\": [-0.5, 1.5],\n                \"autorange\": \"reversed\",\n                \"zeroline\": False,\n            },\n            height=400,\n            plot_bgcolor=\"rgba(240, 240, 240, 0.5)\",\n            margin={\"l\": 150, \"r\": 50, \"t\": 50, \"b\": 100},\n        )\n\n        # Add grid lines\n        fig.add_shape(\n            type=\"line\", x0=-0.5, x1=1.5, y0=0.5, y1=0.5, line={\"color\": \"gray\", \"width\": 1}\n        )\n        fig.add_shape(\n            type=\"line\", x0=0.5, x1=0.5, y0=-0.5, y1=1.5, line={\"color\": \"gray\", \"width\": 1}\n        )\n\n        return fig\n\n\nwith ui.card():\n    ui.h4(\"Confusion Waffle Chart\")\n\n    @render_plotly\n    def confusion_waffle() -> Figure:\n        \"\"\"Create a waffle chart representation of the confusion matrix.\"\"\"\n        counts, _ = current_counts_and_metrics()\n\n        # Create unit squares for each count\n        total = counts.tp + counts.tn + counts.fp + counts.fn\n\n        # Create a 20x20 grid (400 squares total)\n        grid_size = 20\n        scale_factor = (grid_size * grid_size) / total\n\n        # Scale counts to grid\n        scaled_tp = int(counts.tp * scale_factor)\n        scaled_tn = int(counts.tn * scale_factor)\n        scaled_fp = int(counts.fp * scale_factor)\n        scaled_fn = int(counts.fn * scale_factor)\n\n        # Adjust for rounding\n        diff = (grid_size * grid_size) - (scaled_tp + scaled_tn + scaled_fp + scaled_fn)\n        scaled_tp += diff\n\n        # Create grid data\n        categories = (\n            [\"TP\"] * scaled_tp + [\"TN\"] * scaled_tn + [\"FP\"] * scaled_fp + [\"FN\"] * scaled_fn\n        )\n\n        # Shuffle for better visualization (optional)\n        # np.random.shuffle(categories)\n\n        # Create coordinates\n        data = []\n        for i, cat in enumerate(categories):\n            x = i % grid_size\n            y = i // grid_size\n            data.append({\"x\": x, \"y\": y, \"category\": cat})\n\n        df = pd.DataFrame(data)\n\n        fig = px.scatter(\n            df,\n            x=\"x\",\n            y=\"y\",\n            color=\"category\",\n            color_discrete_map={\"TP\": \"#2ca02c\", \"TN\": \"#1f77b4\", \"FP\": \"#d62728\", \"FN\": \"#ff7f0e\"},\n            hover_data={\"category\": True},\n        )\n\n        fig.update_traces(marker={\"size\": 15, \"symbol\": \"square\"})\n\n        fig.update_layout(\n            showlegend=True,\n            xaxis={\"visible\": False},\n            yaxis={\"visible\": False},\n            height=400,\n            plot_bgcolor=\"white\",\n        )\n\n        return fig\n", "type": "text"}, {"name": "utils_confusion.py", "content": "# utils_confusion.py\n\"\"\"Utility functions and classes for confusion matrix analysis.\n\nThis module provides:\n- ConfusionCounts: dataclass for storing 2x2 confusion matrix counts\n- ConfusionMetrics: dataclass for storing computed diagnostic metrics\n- generate_synthetic_scores: create synthetic datasets for testing\n- compute_confusion_counts: calculate TP, FP, TN, FN from scores and labels\n- compute_metrics: calculate standard diagnostic metrics from counts\n- format_counts_table: format confusion matrix for display\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\n\nimport numpy as np\nimport pandas as pd\n\n\n@dataclass\nclass ConfusionCounts:\n    \"\"\"Counts for a 2x2 confusion matrix.\n\n    Attributes:\n    tp : int\n        True positives - correctly predicted positive cases.\n    fn : int\n        False negatives - positive cases incorrectly predicted as negative.\n    fp : int\n        False positives - negative cases incorrectly predicted as positive.\n    tn : int\n        True negatives - correctly predicted negative cases.\n    \"\"\"\n\n    tp: int\n    fn: int\n    fp: int\n    tn: int\n\n    @property\n    def total(self) -> int:\n        \"\"\"Return the total count of all cases in the confusion matrix.\n\n        Returns:\n        int\n            Sum of true positives, false negatives, false positives, and true negatives.\n        \"\"\"\n        return self.tp + self.fn + self.fp + self.tn\n\n\n@dataclass\nclass ConfusionMetrics:\n    \"\"\"Computed diagnostic metrics from confusion matrix counts.\n\n    Attributes:\n    sensitivity : float\n        True positive rate (recall) - proportion of actual positives correctly identified.\n    specificity : float\n        True negative rate - proportion of actual negatives correctly identified.\n    precision : float\n        Positive predictive value - proportion of predicted positives that are actual positives.\n    npv : float\n        Negative predictive value - proportion of predicted negatives that are actual negatives.\n    accuracy : float\n        Overall proportion of correct predictions.\n    prevalence : float\n        Proportion of actual positive cases in the population.\n    \"\"\"\n\n    sensitivity: float\n    specificity: float\n    precision: float\n    npv: float\n    accuracy: float\n    prevalence: float\n\n\ndef generate_synthetic_scores(\n    n_pos: int = 200,\n    n_neg: int = 200,\n    pos_mean: float = 1.5,\n    neg_mean: float = 0.0,\n    pos_std: float = 1.0,\n    neg_std: float = 1.0,\n    random_state: int | None = 0,\n) -> pd.DataFrame:\n    \"\"\"Generate a simple synthetic dataset of scores.\n\n    - disease present (label = 1)\n    - disease absent (label = 0)\n\n    Returns a DataFrame with columns:\n    - score: float\n    - label: 0 or 1\n    \"\"\"\n    rng = np.random.default_rng(random_state)\n\n    pos_scores = rng.normal(loc=pos_mean, scale=pos_std, size=n_pos)\n    neg_scores = rng.normal(loc=neg_mean, scale=neg_std, size=n_neg)\n\n    df_pos = pd.DataFrame({\"score\": pos_scores, \"label\": 1})\n    df_neg = pd.DataFrame({\"score\": neg_scores, \"label\": 0})\n\n    return pd.concat([df_pos, df_neg], ignore_index=True)\n\n\ndef compute_confusion_counts(\n    scores: np.ndarray,\n    labels: np.ndarray,\n    threshold: float,\n) -> ConfusionCounts:\n    \"\"\"Compute TP, FP, TN, FN given scores, labels, and a decision threshold.\n\n    By convention here:\n    - score >= threshold -> predicted positive\n    - score < threshold  -> predicted negative\n    \"\"\"\n    predicted_positive = scores >= threshold\n    predicted_negative = scores < threshold\n\n    actual_positive = labels == 1\n    actual_negative = labels == 0\n\n    tp = int(np.sum(predicted_positive & actual_positive))\n    fn = int(np.sum(predicted_negative & actual_positive))\n    fp = int(np.sum(predicted_positive & actual_negative))\n    tn = int(np.sum(predicted_negative & actual_negative))\n\n    return ConfusionCounts(tp=tp, fn=fn, fp=fp, tn=tn)\n\n\ndef safe_divide(numerator: float, denominator: float) -> float:\n    \"\"\"Return numerator / denominator, or float('nan') if denominator is zero.\"\"\"\n    if denominator == 0:\n        return float(\"nan\")\n    return numerator / denominator\n\n\ndef compute_metrics(counts: ConfusionCounts) -> ConfusionMetrics:\n    \"\"\"Compute standard diagnostic metrics from confusion counts.\n\n    Returns values as proportions (0.0 to 1.0).\n    You can convert to percentages in the app.\n    \"\"\"\n    tp = counts.tp\n    fn = counts.fn\n    fp = counts.fp\n    tn = counts.tn\n    total = counts.total\n\n    sensitivity = safe_divide(tp, tp + fn)\n    specificity = safe_divide(tn, tn + fp)\n    precision = safe_divide(tp, tp + fp)\n    npv = safe_divide(tn, tn + fn)\n    accuracy = safe_divide(tp + tn, total)\n    prevalence = safe_divide(tp + fn, total)\n\n    return ConfusionMetrics(\n        sensitivity=sensitivity,\n        specificity=specificity,\n        precision=precision,\n        npv=npv,\n        accuracy=accuracy,\n        prevalence=prevalence,\n    )\n\n\ndef format_counts_table(counts: ConfusionCounts) -> pd.DataFrame:\n    \"\"\"Build a 2x2 confusion matrix table for display.\n\n                Disease Present   Disease Absent\n    Test +\n    Test -\n    \"\"\"\n    return pd.DataFrame(\n        {\n            \"\": [\"Test Positive\", \"Test Negative\"],\n            \"Disease Present\": [counts.tp, counts.fn],\n            \"Disease Absent\": [counts.fp, counts.tn],\n        }\n    )\n", "type": "text"}]